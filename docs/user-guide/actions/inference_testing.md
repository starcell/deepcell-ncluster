# Evaluating Experiments with Inference Testing

Nauta provides you with the ability to test your trained model using TensorFlow Serving and OpenVINO Model Server (OVMS).  OVMS is an OpenVINO serving component intended to provide hosting for the OpenVINO inference runtime.

For guidance on using _Inference Testing_ to evaluate an experiment, refer to the topics shown below.

- For `nctl predict` command, its subcommands, and parameter information, refer to [predict Commands](predict.md).

For How-to instructions for TensorFlow Serving:

- Refer to [Batch Inference Example](batch_inf_example.md) for running batch inference.

- Refer to [Stream Inference Example](streaming_inference.md) for running streaming inference.

To run prediction on OpenVINO Model Server, refer to [Inference Example on OpenVINO Model Server](openvino_inf.md)

----------------------

## Return to Start of Document

* [README](../README.md)
----------------------

